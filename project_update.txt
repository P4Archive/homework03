---
title: Project Update
author: Eric Campbell and Dietrich Geisler
date: November 21, 2017
publish: true
urlcolor: blue
---

We will be re-implementing and measuring HashPipe [(link)][1]. This algorithm
approximately calculates the top heavy-hitters on a switch. To do
this, it defines a pipeline of `n` hash tables that record the
byte-counts for each incoming flow. Of course, these tables are
finite, so it cannot actually record every flow, and so we must
specify a collision and eviction scheme.

### Actions (Control Flow)

We define two different kinds of actions, `hash` and `pipe`:
the `hash` action handles flows as they come in and acts as a MRU
cache; the `pipe` actions perform the heavy-hitters
calculations. There is only one `hash` action, but there may be
arbitrarily many `pipe` actions.

The `hash` action always accepts the incoming packet. If there is a
collision it writes-back the old value to the first `pipe` action. It,
as its name implies computes the hash function and stores the result
and the count in new packet `metadata` fields called `keyCarry` and
`countCarry`.

In the `pipe` action, we accept writes when there is no
collision. When a packet collides with another in a hash table, the
flow that has the greater byte-count is left in place, and the one
that has the lesser is written back to the next hash table in the
pipeline. If the flows are, in fact the same, then the counts are
summed. If the current `pipe` action is the last one, we drop the
packet count from the table. Hash tables are not implemented as
switch-level `tables` but as _register arrays_.

Our pipeline mimics the example given in the paper and uses three
stages.

### Evaluation

As a sanity check, we will instantiate the hash tables with size one,
and create a simple topology of five hosts `h1,h2,h3,h4,h5` connected
via one switch. It then assigns `h1` to be a heavy hitter, and `h5` to
be the "server". We bombard `h5` with traffic from `h1,...,h4`
ensuring that `h1` sends twice as much traffic as `h2,...,h4`. We then
check the final table to make sure that `h1` is in one of the tables
and has the highest count.

For a more robust test of correctness, we generalize to 300 client
hosts and tables of size 50 and have 1/3 of the clients be
ground-truth heavy-hitters. We hope to see that the HashPipe results
are close to the ground-truth.

To measure efficiency we will simply measure the latency of the switch
over heavy `iperf` loads for each of topology defined above.

[1]: https://conferences.sigcomm.org/sosr/2017/papers/sosr17-heavy-hitter.pdf
